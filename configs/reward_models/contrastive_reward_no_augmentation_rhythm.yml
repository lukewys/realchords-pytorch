# Trainer hyperparameters, for Trainer and lit_module
seed: 42
train_steps: 8000
val_interval: 100
sample_interval: 1000
max_log_examples: 8
overfit_batches: 0
wandb_project: "realchords"
log_every_n_steps: 1
checkpoint_interval: 500
checkpoint_metric: "val/loss"
checkpoint_mode: "min"
checkpoint_top_k: -1 # save all checkpoints
accelerator: "auto"
devices: "auto"
num_nodes: 1
precision: "bf16-mixed"
strategy: "auto"
compile: True

# Data hyperparameters, for get_dataloader
batch_size: 196
num_workers: 8
cache_dir: "data/cache"
data_augmentation: false
load_augmented_chord_names: true

# Dataloader hyperparameters, for HooktheoryDataset
model_part: "chord"
max_len: 512
model_type: "encoder_decoder"

# Optimizer hyperparameters
AdamW.lr: 0.0001
AdamW.weight_decay: 0.0

# Model hyperparameters
ContrastiveReward.dim: 512
ContrastiveReward.depth: 6
ContrastiveReward.heads: 6
ContrastiveReward.max_seq_len: 258 # bos, eos
EncoderDecoderTransformer.dropout: 0.1
