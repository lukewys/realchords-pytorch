# Trainer hyperparameters, for Trainer and lit_module
seed: 42
train_steps: 30000
val_interval: 1000
sample_interval: 5000
overfit_batches: 0
wandb_project: "realchords"
log_every_n_steps: 1
checkpoint_interval: 1000
checkpoint_metric: "val/loss"
checkpoint_mode: "min"
checkpoint_top_k: -1 # save all checkpoints
accelerator: "auto"
devices: "auto"
num_nodes: 1
precision: "bf16-mixed"
strategy: "auto"
compile: True

# Data hyperparameters, for get_dataloader
batch_size: 64
num_workers: 8
cache_dir: "data/cache"

# Dataloader hyperparameters, for HooktheoryDataset
model_part: "chord"
max_len: 512
model_type: "decoder_only"

# Optimizer hyperparameters
AdamW.lr: 0.0001
AdamW.weight_decay: 0.0

# Model hyperparameters
DecoderTransformer.dim: 512
DecoderTransformer.depth: 8
DecoderTransformer.heads: 8
DecoderTransformer.max_seq_len: 514 # bos, eos
DecoderTransformer.dropout: 0.1
